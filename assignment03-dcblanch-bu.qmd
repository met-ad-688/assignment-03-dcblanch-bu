---
title: Assignment 03
author:
  - name: Devin Blanchard
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-24'
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx: default
  pdf: default
date-modified: today
date-format: long
execute:
  echo: true
  eval: true
  freeze: auto
--- 


## Data Loading and Inspection

```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id

np.random.seed(42)

pio.renderers.default = "notebook"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

df.printSchema() # comment this line when rendering the submission
df.show(5)

```


## Data Cleaning

```{python}
from pyspark.sql.functions import col

df = df.withColumn("SALARY", col("SALARY").cast("double"))
df = df.withColumn("SALARY_FROM", col("SALARY_FROM").cast("double"))
df = df.withColumn("SALARY_TO", col("SALARY_TO").cast("double"))
df = df.withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("double"))
df = df.withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("double"))

# median_from, median_to, median_salary
median_from = df.approxQuantile("SALARY_FROM", [0.5], 0.01)[0]
median_to = df.approxQuantile("SALARY_TO", [0.5], 0.01)[0]
median_salary = df.approxQuantile("SALARY", [0.5], 0.01)[0]

print(median_from, median_to, median_salary)

from pyspark.sql.functions import when, isnan

# Compute average salary column
df = df.withColumn(
    "AVERAGE_SALARY",
    ((col("SALARY_FROM") + col("SALARY_TO")) / 2)
)

# Impute SALARY
df = df.withColumn(
    "SALARY",
    when(
        col("SALARY").isNull(),
        when(col("AVERAGE_SALARY").isNotNull(), col("AVERAGE_SALARY"))
        .otherwise(median_salary)
    ).otherwise(col("SALARY"))
)

from pyspark.sql.functions import regexp_replace

df = df.withColumn(
    "EDUCATION_LEVELS_NAME",
    regexp_replace(col("EDUCATION_LEVELS_NAME"), r'[\n\r]', '')
)

# Adjust path as needed (overwrite mode)
df.write.option("header", True).mode("overwrite").csv("../data/lightcast_job_postings_cleaned.csv")

# Display row count
print(f"Rows retained after cleaning: {df.count()}")

```

## Salary Distribution by Industry and Employment Type

```{python}
import plotly.express as px

# Filter for non-missing, nonzero salaries (use SALARY, not SALARY_FROM)
plot_df = df.select("NAICS2_NAME", "SALARY").filter(col("SALARY") > 0).toPandas()

fig = px.box(
    plot_df,
    x="NAICS2_NAME",
    y="SALARY",
    points="all",        # Show all points like example
    title="Salary Distribution by Industry",
)

fig.update_traces(
    marker=dict(color='rgb(255,87,51)', opacity=0.5),
    line=dict(color='rgb(255,87,51)'),
    fillcolor='rgba(255,87,51,0.3)'
)

fig.update_layout(
    xaxis_title="Industry",
    yaxis_title="SALARY",
    font=dict(size=14, family="Arial"),
    plot_bgcolor="#F4F8FF",
    paper_bgcolor="#F4F8FF",
    xaxis_tickangle=-45,
    height=700,
    width=1200,
    showlegend=False
)

fig.show()

```

Salaries vary widely between industries, with sectors like Information and Finance & Insurance generally showing higher salary ranges than industries such as Accommodation and Food Services. Full-time positions tend to have higher median salaries across most industries compared to part-time or other employment types.