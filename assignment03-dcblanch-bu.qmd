---
title: Assignment 03
author:
  - name: Devin Blanchard
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-24'
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx: default
  pdf: default
date-modified: today
date-format: long
execute:
  echo: true
  eval: true
  freeze: auto
--- 


## Data Loading and Inspection

```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id

np.random.seed(42)

pio.renderers.default = "notebook"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

df.printSchema() # comment this line when rendering the submission
df.show(5)

```


## Data Cleaning

```{python}
# Cast salary columns
from pyspark.sql.functions import col

salary_cols = ["SALARY_FROM", "SALARY_TO", "SALARY"]
exp_cols = ["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE"]

for c in salary_cols + exp_cols:
    df = df.withColumn(c, col(c).cast("double"))

# Compute medians
median_from = df.approxQuantile("SALARY_FROM", [0.5], 0.01)[0]
median_to = df.approxQuantile("SALARY_TO", [0.5], 0.01)[0]
median_salary = df.approxQuantile("SALARY", [0.5], 0.01)[0]

print("Medians:", median_from, median_to, median_salary)

from pyspark.sql.functions import when

# Impute missing salaries using medians
df = (
    df.withColumn("SALARY_FROM", when(col("SALARY_FROM").isNull(), median_from).otherwise(col("SALARY_FROM")))
      .withColumn("SALARY_TO", when(col("SALARY_TO").isNull(), median_to).otherwise(col("SALARY_TO")))
      .withColumn("SALARY", when(col("SALARY").isNull(), median_salary).otherwise(col("SALARY")))
)

# Add average salary column
df = df.withColumn("AVERAGE_SALARY", (col("SALARY_FROM") + col("SALARY_TO")) / 2)

# Show sample with new columns
df.select("AVERAGE_SALARY", "SALARY", "SALARY_FROM", "SALARY_TO").show(5)


# Clean Education column
from pyspark.sql.functions import regexp_replace, trim

# Remove \n and \r characters and extra spaces
df = df.withColumn(
    "EDUCATION_LEVELS_NAME",
    trim(regexp_replace(col("EDUCATION_LEVELS_NAME"), r'[\n\r]', ''))
)

# Save cleaned data to CSV
df.write.mode("overwrite").option("header", "true").csv("data/lightcast_job_postings_cleaned.csv")

# Check row count
row_count = df.count()
print("Rows retained:", row_count)

```